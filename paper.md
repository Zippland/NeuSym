# Small Language Models Enhanced by Neural-Symbolic Reasoning

## Abstract  
Neural language models have achieved remarkable success in natural language processing, but they often struggle with tasks requiring logical reasoning and structured knowledge manipulation. This paper proposes a neuro-symbolic approach to enhance a **small language model (LM)** with explicit reasoning capabilities. We design a hybrid **neural-symbolic framework** that tightly integrates a neural network language model with a symbolic logic engine. The neural component handles natural language understanding and extraction of facts, while the symbolic component carries out rule-based inference, guided by a **hybrid reasoning controller**. We develop an interface to translate between natural language and logical representations, enabling the system to perform multi-step reasoning on tasks that pure neural models find challenging. Our contributions include: (1) an architecture for neural-symbolic fusion that leverages a small LM for analogical reasoning and a logic engine for discrete reasoning; (2) a method for converting raw text into logical forms and vice versa, maintaining alignment between neural embeddings and symbolic facts; (3) a mixed reasoning controller that dynamically decides when to invoke neural or symbolic modules; and (4) an explainable inference mechanism using a rule-based engine, improving result transparency. Experiments on synthetic and semi-synthetic reasoning benchmarks (including the bAbI QA tasks) demonstrate that our neuro-symbolic model outperforms purely neural baselines in accuracy, particularly on tasks requiring multi-hop reasoning or precise logical deduction. We achieve near-perfect accuracy on most bAbI tasks, with significant gains on the challenging tasks of induction and path-finding. The integrated approach also offers improved **interpretability** – the system can produce explicit reasoning traces – at the cost of a moderate increase in inference time. These results highlight the promise of combining neural and symbolic methods for complex reasoning. We discuss limitations, such as performance bottlenecks in the logic engine for very large knowledge bases, and provide insights into optimizing the neural-symbolic interface. Finally, we outline future directions including incorporation of external knowledge graphs to further boost reasoning, adaptive controllers for balancing neural vs. symbolic computation, and extensions to real-world domains like medical or legal reasoning. This work suggests that even relatively small LMs can tackle complex reasoning tasks when complemented by symbolic structures, pointing toward more **explainable and reliable AI** for the next generation of natural language understanding systems.

## Introduction  
Large-scale neural language models have revolutionized NLP, but they still exhibit fundamental limitations in reasoning and interpretability ([[2203.10557] A Neural-Symbolic Approach to Natural Language Understanding](https://arxiv.org/abs/2203.10557#:~:text=,analogical%20reasoning%20based%20on%20neural)) ([[2205.00445] MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning](https://arxiv.org/abs/2205.00445#:~:text=,system)). Pure neural models excel at pattern recognition and learning from data, yet they often **struggle with logical inference** and multi-step reasoning tasks that require consistent manipulation of symbols or facts ([[1912.05421] Just Add Functions: A Neural-Symbolic Language Model](https://ar5iv.org/pdf/1912.05421#:~:text=hypothesis%20of%20language,neural%20models%20with%20such%20encodings)) ([[2203.10557] A Neural-Symbolic Approach to Natural Language Understanding](https://arxiv.org/abs/2203.10557#:~:text=,both%20neural%20and%20symbolic%20processing)). For example, transformer-based models can generate fluent text, but may fail to consistently apply rules of arithmetic or transitive relations in a story. These shortcomings become more pronounced for **small language models** (with fewer parameters or trained on limited data), which lack the brute-force memorization capacity of extremely large models. As a result, small LMs can have difficulty generalizing to tasks like temporal reasoning, commonsense chaining of facts, or deductive question answering without support.

Human cognition, in contrast, combines intuitive pattern recognition with deliberate logical reasoning – often described by **dual-process theory** as System 1 (fast, intuitive thinking) and System 2 (slow, logical thinking) ([[2203.10557] A Neural-Symbolic Approach to Natural Language Understanding](https://arxiv.org/abs/2203.10557#:~:text=is%20needed,a%20type%20of)). Inspired by this, researchers are exploring **neuro-symbolic AI** approaches that integrate neural networks (for perception and intuitive tasks) with symbolic systems (for logic and reasoning) ([Q&A: Can Neuro-Symbolic AI Solve AI’s Weaknesses? | TDWI](https://tdwi.org/Articles/2024/04/08/ADV-ALL-Can-Neuro-Symbolic-AI-Solve-AI-Weaknesses.aspx#:~:text=Neuro,classification%2C%20prediction%2C%20and%20contextual%20understanding)) ([SmythOS - Symbolic AI Frameworks: Introduction to Key Concepts](https://smythos.com/ai-agents/agent-architectures/symbolic-ai-frameworks/#:~:text=The%20third%20key%20component%20involves,to%20make%20final%20diagnostic%20recommendations)). The intuition is that a neural component can interpret raw data (text, images, etc.) and propose candidate inferences, while a symbolic component can enforce **logical consistency**, manipulate symbols like variables and relations, and provide **explainable** reasoning chains ([Q&A: Can Neuro-Symbolic AI Solve AI’s Weaknesses? | TDWI](https://tdwi.org/Articles/2024/04/08/ADV-ALL-Can-Neuro-Symbolic-AI-Solve-AI-Weaknesses.aspx#:~:text=The%20features%20of%20neuro,ones%20that%20come%20to%20mind)) ([Using symbolic AI for knowledge-based question answering - IBM Research](https://research.ibm.com/blog/ai-neurosymbolic-common-sense#:~:text=Our%20NSQA%20achieves%20state,out%20the%20steps%20of%20reasoning)). Such integration seeks to combine the strengths of both paradigms: the robust learning and generalization from examples by neural nets, and the **clarity, correctness, and generalization** of symbolic reasoning over structured knowledge ([SmythOS - Symbolic AI Frameworks: Introduction to Key Concepts](https://smythos.com/ai-agents/agent-architectures/symbolic-ai-frameworks/#:~:text=creates%20a%20powerful%20combination%E2%80%94symbolic%20AI,to%20make%20final%20diagnostic%20recommendations)). A well-designed neuro-symbolic system could solve complex problems with limited data, and do so in a way that is more interpretable and verifiable than a black-box neural network alone ([Q&A: Can Neuro-Symbolic AI Solve AI’s Weaknesses? | TDWI](https://tdwi.org/Articles/2024/04/08/ADV-ALL-Can-Neuro-Symbolic-AI-Solve-AI-Weaknesses.aspx#:~:text=mind%3A)).

**Symbolic reasoning** is particularly relevant for overcoming the weaknesses of neural LMs in tasks requiring stepwise deduction, rule application, or combinatorial generalization. Prior works have noted that neural LMs often cannot reliably learn certain **spatial, temporal, or quantitative relationships** expressed in language, even though these may be trivial for humans or can be captured by simple logic ([[1912.05421] Just Add Functions: A Neural-Symbolic Language Model](https://ar5iv.org/pdf/1912.05421#:~:text=hypothesis%20of%20language,neural%20models%20with%20such%20encodings)). For instance, a small LM might be confused by a story involving multiple characters moving between locations and then be asked “Where is person X now?” – something requiring tracking the latest location of X. Pure sequence-to-sequence models might answer incorrectly if they fail to implicitly learn the state update rule, whereas a symbolic representation of the state (e.g. a table of each person’s location) would make the query trivial. **Neural models lack an explicit mechanism to store and update such structured state or to enforce global consistency of their outputs** ([[1912.05421] Just Add Functions: A Neural-Symbolic Language Model](https://ar5iv.org/pdf/1912.05421#:~:text=encodings%3F)). They are also **data-hungry** – requiring many examples to learn patterns – and even then may not extrapolate well beyond the training distribution (for example, failing to generalize arithmetic to numbers not seen during training). By contrast, a symbolic engine can apply the same rule to any novel instance (e.g., transitivity of “larger than” for any new set of objects).

Another critical limitation is **interpretability**. Neural networks operate as black boxes, making it hard to understand why a certain answer was produced or to trust it in sensitive applications ([Q&A: Can Neuro-Symbolic AI Solve AI’s Weaknesses? | TDWI](https://tdwi.org/Articles/2024/04/08/ADV-ALL-Can-Neuro-Symbolic-AI-Solve-AI-Weaknesses.aspx#:~:text=,large%20data%20sets%20to%20learn)). Symbolic systems, on the other hand, can provide human-readable proofs or rule traces. Merging these approaches offers a path to more explainable AI: the neural part can convert raw inputs into structured form, and the symbolic part can then derive conclusions in a way that can be inspected and verified ([Using symbolic AI for knowledge-based question answering - IBM Research](https://research.ibm.com/blog/ai-neurosymbolic-common-sense#:~:text=Our%20NSQA%20achieves%20state,out%20the%20steps%20of%20reasoning)).

In this paper, we address the above challenges by proposing a **hybrid neural-symbolic framework** that enhances a small language model with symbolic reasoning capabilities. Instead of relying on an enormous neural network to implicitly learn everything, we explicitly **delegate** the heavy logical lifting to a symbolic module. Our approach allows a relatively small LM to punch above its weight on reasoning tasks, as the symbolic component compensates for the LM’s weaknesses in logic. The key idea is to have the LM and the logic engine work in tandem: the LM interprets natural language and provides a bridge to formal representations, while the logic engine, armed with rules or constraints, performs exact inference on those representations.

Concretely, our system consists of a **neural-symbolic interface** that translates natural language (sentences, questions) into a logical form (such as facts and queries in a knowledge base), a **symbolic reasoning engine** (which could be a rule-based prover or a logic programming module) that operates on these facts, and a **controller** that orchestrates the interaction between the two. The controller decides how to combine neural inference with symbolic inference – for example, whether to answer a query directly from the neural model’s knowledge or to consult the logical reasoner for multi-hop deduction. By combining subsymbolic and symbolic reasoning, the system can answer questions that are beyond the reach of a standalone neural model, while still leveraging the neural model’s strengths in understanding language.

Our approach builds on and extends prior research in neuro-symbolic AI. Early work by d’Avila Garcez _et al._ highlighted the vision of integrating robust neural learning with logical reasoning to achieve **explainable and sound AI systems** ([[1905.06088] Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning](https://ar5iv.org/pdf/1905.06088#:~:text=foreseen%20by%20Valiant%2C%20two%20most,the%20construction%20of%20explainable%20AI)) ([[1905.06088] Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning](https://ar5iv.org/pdf/1905.06088#:~:text=learned.%20Neural,computing%20shed%20new%20light%20on)). There have been various strategies in the literature: some methods inject symbolic knowledge into neural networks as additional constraints or regularizers (e.g. logic regularization, differentiable satisfiability) ([[1805.10872] DeepProbLog: Neural Probabilistic Logic Programming](https://arxiv.org/abs/1805.10872#:~:text=,end%20based%20on%20examples)); others perform a pipeline where neural models first extract information which is then fed to a classical reasoner ([Using symbolic AI for knowledge-based question answering - IBM Research](https://research.ibm.com/blog/ai-neurosymbolic-common-sense#:~:text=Next%2C%20we%E2%80%99ve%20used%20LNNs%20to,base%20to%20produce%20the%20answer)); yet others attempt a tight coupling via end-to-end differentiable architectures that mimic logic (like Neural Theorem Provers or differentiable logic programming) ([[1805.10872] DeepProbLog: Neural Probabilistic Logic Programming](https://arxiv.org/abs/1805.10872#:~:text=existing%20inference%20and%20learning%20techniques,end%20based%20on%20examples)) ([[1904.11694] Neural Logic Machines](https://arxiv.org/abs/1904.11694#:~:text=,decision%20making%20tasks%20including%20sorting)). Each approach has pros and cons. Fully end-to-end differentiable neurosymbolic systems (e.g. DeepProbLog ([[1805.10872] DeepProbLog: Neural Probabilistic Logic Programming](https://arxiv.org/abs/1805.10872#:~:text=,exploits%20the%20full%20expressiveness%20and)) or Neural Logic Machines ([[1904.11694] Neural Logic Machines](https://arxiv.org/abs/1904.11694#:~:text=,decision%20making%20tasks%20including%20sorting))) can in principle learn seamlessly from data, but they often require complex training and can struggle to scale or to handle raw language input directly. Pipeline approaches with a clear separation (neural for perception, symbolic for reasoning) may sacrifice some joint optimization, but gain in modularity and interpretability. Our work leans toward the latter: we prioritize a clean interface between the neural and symbolic parts so that each can be optimized and understood in its own right, and the overall system’s decisions can be traced.

To evaluate our framework, we conduct experiments on the **bAbI dataset** ([](https://cs229.stanford.edu/proj2015/333_report.pdf#:~:text=1%20Dataset%20The%20Facebook%20bAbI,objects%20interacting%20in%20a%20small)) – a set of 20 synthetic QA tasks that require reasoning in natural language contexts – as well as other reasoning challenges. The bAbI tasks are ideal for our purposes: they are designed to test capabilities like remembering basic facts, chain reasoning, counting, deduction, induction, and so on ([](https://cs229.stanford.edu/proj2015/333_report.pdf#:~:text=The%20Facebook%20bAbI%20dataset%20is,performing%20deduction%20over%20multiple%20sentences)), which small LMs often struggle with. We show that our neural-symbolic model achieves **higher accuracy** on these tasks compared to a purely neural baseline of similar size. In particular, on tasks that involve multi-step inference or the application of rules (e.g. transitive relations, set reasoning, or positional reasoning), the neuro-symbolic approach yields substantial improvements. For example, a basic LSTM or memory-network-based model might only solve 15 out of 20 tasks with >95% accuracy ([Optimizing End-to-End Memory Networks Using SigOpt and GPUs | NVIDIA Technical Blog](https://developer.nvidia.com/blog/optimizing-end-to-end-memory-networks-using-sigopt-gpus/#:~:text=,on%20these%20qualities%20to%20impact)), whereas our model solves nearly all tasks at that level. Moreover, our system provides **interpretable reasoning chains** for its answers, which is unheard of in standard neural models. We measure the trade-off in efficiency and find that while the symbolic reasoning adds some overhead, the overall response time remains reasonable for short texts (on the order of a few hundred milliseconds per query in our implementation).

The remainder of this paper is organized as follows. In **Related Work**, we review the landscape of neural-symbolic reasoning research, comparing existing methods and highlighting what distinguishes our approach. In **Methodology**, we detail the design of our neural-symbolic framework: the architecture of the neural and symbolic components, the interface for translating between natural language and logic, the hybrid reasoning controller’s algorithm, and the logic inference engine we employ. The **Experiments and Evaluation** section describes the datasets (bAbI and others), experimental setup, and evaluation metrics (accuracy, reasoning time, interpretability). We then present results, including comparisons to baseline models and ablation studies to understand each component’s contribution. In **Results Analysis**, we delve into the system’s performance, discussing why the neuro-symbolic model excels on certain tasks, where it falls short, and analyzing specific examples of reasoning traces. We also identify bottlenecks such as error propagation from the neural parser or performance issues in the reasoning engine, and suggest optimizations. In **Future Work**, we outline avenues for extending this research: incorporating knowledge graphs to provide broader world knowledge, making the reasoning process adaptive or learned, and applying the method to complex domains like healthcare or legal reasoning which require integrating rules with learned knowledge. Finally, the **Conclusion** summarizes our findings and reflects on the implications for the development of **next-generation AI** that can both learn from data and reason with knowledge, even at small model scales.

## Related Work  
Combining neural networks with symbolic reasoning has been a long-standing research quest, often termed **neural-symbolic computing** ([[1905.06088] Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning](https://ar5iv.org/pdf/1905.06088#:~:text=foreseen%20by%20Valiant%2C%20two%20most,neural%20learning%20with%20symbolic%20knowledge)). Early efforts in the 1990s and 2000s (e.g., Garcez & Zaverucha 1999, Hitzler et al. 2004) sought to encode logical knowledge into the weights of neural networks or to extract logical rules from trained networks ([[1905.06088] Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning](https://ar5iv.org/pdf/1905.06088#:~:text=learning%20meets%20probabilistic%20dbs,Reasoning%20about%20time%20and%20knowledge)). These laid the groundwork for modern approaches, which can be broadly categorized into: (1) methods that **embed symbolic knowledge into neural models** (through regularization, loss functions, or architecture design), (2) methods that **extract neural outputs into symbolic form for downstream reasoning**, and (3) fully **integrated neuro-symbolic architectures** that intermix differentiable and symbolic components.

**Symbolic regularization and constraints:** One line of research augments neural network training with symbolic logic constraints so that the learned model respects certain rules. For example, Xu _et al._ (2018) and others introduced approaches to relax logical formulas into differentiable loss terms that penalize violations of domain knowledge ([Efficient Rectification of Neuro-Symbolic Reasoning Inconsistencies ...](https://arxiv.org/html/2412.08457v1#:~:text=Efficient%20Rectification%20of%20Neuro,2018%3B%20Yang%2C%20Lee%2C%20and)). An application is in visual question answering or relation extraction, where a rule like “If X is Y’s parent and Y is Z’s parent, then X is Z’s grandparent” can be enforced during training so that the model’s predictions become logically consistent. Such methods keep reasoning implicit within the neural model but guide it with symbolic structure. A downside is that they require specifying the rules upfront and the resulting model is still a black box – it’s hard to extract explicit reasoning despite improved consistency.

**Neural theorem provers and differentiable reasoning:** Another direction tries to make the reasoning itself differentiable. **Neural Theorem Prover (NTP)** by Rocktäschel & Riedel (2017) and subsequent works attempt to learn how to prove queries by unifying embeddings of symbols ([Reviews: DeepProbLog: Neural Probabilistic Logic Programming](https://proceedings.neurips.cc/paper_files/paper/2018/file/dc5d637ed5e62c36ecb73b654b05ba2a-Reviews.html#:~:text=DeepProbLog%20differs%20because%20logical%20reasoning,but%20uses%20a%20language)). These models can learn symbolic rules from data (a form of differentiable inductive logic programming). Similarly, **DeepProbLog** ([[1805.10872] DeepProbLog: Neural Probabilistic Logic Programming](https://arxiv.org/abs/1805.10872#:~:text=,exploits%20the%20full%20expressiveness%20and)) integrated neural predicates into the probabilistic logic programming language ProbLog. DeepProbLog can call a neural network as a sub-routine within a logic program – for instance, use a CNN to probabilistically recognize MNIST digits, then reason about their sum in logic ([[1805.10872] DeepProbLog: Neural Probabilistic Logic Programming](https://arxiv.org/abs/1805.10872#:~:text=,end%20based%20on%20examples)). It retains the soundness of logic programming while benefiting from neural perception. Such systems elegantly combine symbolic and subsymbolic inference and can be trained end-to-end, but they are typically limited to relatively small problem sizes due to the complexity of reasoning over many possibilities (the neural networks make the search space probabilistic but not exponential, yet scaling remains challenging). They also usually assume the input to the reasoning part is already in a structured form (e.g., extracted symbols or features), thus not solving the problem of converting raw text to logic.

**Hierarchical or modular architectures:** A number of researchers have proposed **modular architectures** where different components handle different parts of the task – a natural approach for neuro-symbolic integration. One notable example is the **Neuro-Symbolic Concept Learner (NS-CL)** by Mao _et al._ (2019) for visual question answering. NS-CL uses a neural perceptual module to extract a scene representation (objects with attributes), a neural parser to convert the question into a functional program (a sequence of operations), and a symbolic program executor that performs the logic (answering the question by executing the program on the scene representation) ([[PDF] the neuro-symbolic concept learner: interpreting scenes, words, and ...](https://jiajunwu.com/papers/nscl_iclr.pdf#:~:text=,of%20sentences%20without%20explicit)) ([[PDF] Neuro-Symbolic Visual Concept Learning](https://cdn.codeground.org/nsr/downloads/global/cambridge-workshop/Neuro-Symbolic%20Visual%20Concept%20Learning-Jiajun%20Wu.pdf#:~:text=%5BPDF%5D%20Neuro,Dynamic%20Concept)). This achieved strong performance on the CLEVR dataset with full interpretability (the program execution trace is an explanation). Our work is analogous in spirit but in the domain of textual reasoning: we use a neural module to parse text into facts and a symbolic executor to reason over those facts.

In NLP, **neuro-symbolic question answering (QA)** systems have been built for knowledge base QA. For instance, **Neuro-Symbolic QA (NSQA)** by IBM research translates a natural language question into a logical query (SPARQL or logical form) and then uses a symbolic reasoner (a knowledge base) to find the answer ([Using symbolic AI for knowledge-based question answering - IBM Research](https://research.ibm.com/blog/ai-neurosymbolic-common-sense#:~:text=Next%2C%20we%E2%80%99ve%20used%20LNNs%20to,base%20to%20produce%20the%20answer)). This approach achieved state-of-the-art on certain knowledge-based QA benchmarks without end-to-end training, relying on the compositional generalization of the symbolic query executor. The logic tool in that case was a *Logical Neural Network (LNN)* reasoning over a knowledge graph ([Using symbolic AI for knowledge-based question answering - IBM Research](https://research.ibm.com/blog/ai-neurosymbolic-common-sense#:~:text=fundamentally%20new%20neuro,with%20domain%20knowledge%20for%20reasoning)). The benefit is that it can provide an explanation of which facts led to the answer, and easily incorporate new facts by updating the knowledge base rather than retraining a neural model. The drawback is the need for a semantic parser; if the parser errs, the whole QA can fail (common in pipeline approaches). Recent advances in large LMs have actually improved semantic parsing via prompt-based methods, but large LMs themselves can sometimes directly answer KB queries if given the right prompt – albeit without guarantee of consistency.

Another example is the **Mastering the Game of Go** approach (DeepMind’s AlphaGo), often cited as a successful hybrid of neural and symbolic (Monte Carlo tree search was guided by neural networks). Although not an NLP task, it exemplifies using a neural network to evaluate states and a symbolic search algorithm to perform long-term planning. In our context, one can think of using a neural model to evaluate or suggest next reasoning steps, and a symbolic search (over knowledge graphs or rule applications) to systematically explore conclusions. There have been attempts to do something similar for logical puzzles and synthetic tasks, often referred to as **neural-guided deductive search**.

**Memory-augmented networks:** Some QA models incorporate a form of symbolic memory. Memory Networks and their successors (End-to-End Memory Networks, Dynamic Memory Networks) can be seen as adding a **soft symbolic component** to neural models ([](https://cs229.stanford.edu/proj2015/333_report.pdf#:~:text=focus%20on%20question%20answering%3A%20Can,1%20Dataset)) ([](https://cs229.stanford.edu/proj2015/333_report.pdf#:~:text=are%20a%20variety%20of%20approaches,al)). They maintain a memory of facts (sentences) and use attention (a differentiable addressing mechanism) to retrieve and reason over those facts to answer questions. In the original bAbI experiments, Memory Networks achieved strong results, especially when given supporting fact supervision ([](https://cs229.stanford.edu/proj2015/333_report.pdf#:~:text=Several%20authors%20have%20studied%20using,CS221%20but%20not%20in%20CS229)). However, purely neural memory networks still treat the memory as a continuous vector store – they don’t perform explicit logical inference, but rather multiple rounds of attention that *simulate* reasoning. Our approach differs in that our memory is a **true symbolic store** (facts in a knowledge base), and inference is done by a logic engine that can, for example, chain together facts or apply a rule of implication. This means we can get exact answers and guaranteed properties (e.g., if the rules are sound, the conclusions are logically entailed), at the cost of needing the symbols in discrete form.

**Neural-Symbolic reasoning on knowledge graphs:** Recent surveys (e.g., Zhang et al. 2022) cover how neural and symbolic methods combine for knowledge graph completion and reasoning ([[PDF] Neurosymbolic AI for Reasoning over Knowledge Graphs: A Survey](https://arxiv.org/pdf/2302.07200#:~:text=,symbolic%20reasoning%20methods%20with%20deep)). **Logic Tensor Networks (LTN)** and related frameworks (Serafini & d’Avila Garcez, 2016) embed first-order logic predicates as differentiable operations in a network, allowing simultaneous learning of embeddings and satisfiability of logical constraints ([GitHub - LAMDASZ-ML/Awesome-Neuro-Symbolic-Learning-with-LLM: ✨✨Latest Advances on Neuro-Symbolic Learning in the era of Large Language Models](https://github.com/LAMDASZ-ML/Awesome-Neuro-Symbolic-Learning-with-LLM#:~:text=MultiplexNet%3A%20Towards%20Fully%20Satisfied%20Logical,Logical%20Constraints%20NeurIPS%202023%20Github)). In a sense, they blur the line by making logic “neural”. Our approach keeps a clearer separation: we use a conventional logic engine (non-differentiable) but integrate it with the neural model’s workflow.

**Dual Process and system bridging:** The idea of **System 1 (neural) and System 2 (symbolic)** has been explicitly explored. For example, Bai et al. (2022) introduced a two-system model for logical reasoning on commonsense knowledge graphs ([Neurosymbolic Systems of Perception and Cognition: The Role of ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC9163389/#:~:text=Neurosymbolic%20Systems%20of%20Perception%20and,and%20they%20are%20often)). Their System 1 produced vector embeddings for entities and relations (intuitive representations), and System 2 applied neural modules implementing logical operators (AND, OR, NOT) to those embeddings to infer new knowledge ([](https://aclanthology.org/2022.findings-emnlp.42.pdf#:~:text=a%20newly%20created%20relation%20%E2%80%9CSIM%E2%80%9D%2C,use%20these%20representations%20to%20compute)) ([](https://aclanthology.org/2022.findings-emnlp.42.pdf#:~:text=be%20predicted%20in%20inference,To%20increase%20coverage%2C%20we)). They report that the two-system architecture improved performance over a pure neural approach on knowledge graph completion ([Neural-Symbolic Chain of Logic Reasoning - ACL Anthology](https://aclanthology.org/2022.findings-emnlp.42/#:~:text=Neural,its%20System%201%20model%20alone)). This resonates with our design, where the small LM (System 1) does quick pattern matching (e.g., finds a directly stated answer or does a single hop retrieval), whereas the symbolic module (System 2) carries out deliberative reasoning (multi-hop, chaining, or applying general rules). Importantly, System 2’s operations can be constrained to follow logical *truth* – e.g., ensuring that if “A entails B” and “B entails C”, then the system infers “A entails C”. This kind of guarantee is hard to enforce in end-to-end neural systems but comes naturally in a symbolic reasoner.

**MRKL systems and tool use by LMs:** A recent development in large language models is the use of **modular tool-using architectures** such as MRKL (Modular Reasoning, Knowledge and Language) systems ([[2205.00445] MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning](https://arxiv.org/abs/2205.00445#:~:text=discuss%20these%20limitations%20and%20how,AI21%20Labs%27%20MRKL%20system%20implementation)). These frameworks allow an LM to delegate subtasks to external modules – such as calculators, knowledge base queries, or search engines – which can be seen as a neuro-symbolic cooperation. Karpas et al. (2022) define the MRKL architecture where a central LM routes parts of a user query to appropriate experts (some could be neural, some symbolic) and then integrates their results ([[2205.00445] MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning](https://arxiv.org/abs/2205.00445#:~:text=discuss%20these%20limitations%20and%20how,AI21%20Labs%27%20MRKL%20system%20implementation)) ([Jurassic-X: Crossing the neuro-symbolic chasm with the MRKL system | AI21](https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system#:~:text=Reasoning%2C%20Knowledge%20and%20Language%20,the%20reach%20of%20neural%20models)). For example, AI21’s **Jurassic-X** system uses a large LM together with symbolic knowledge tools to answer questions requiring up-to-date knowledge or exact computation ([Jurassic-X: Crossing the neuro-symbolic chasm with the MRKL system | AI21](https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system#:~:text=What%20is%20a%20MRKL%20system%3F)) ([Jurassic-X: Crossing the neuro-symbolic chasm with the MRKL system | AI21](https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system#:~:text=to%20current%20information%20,Here%E2%80%99s%20how%20it%20works)). **Figure 1** illustrates this concept: the LM breaks a complex query into parts, consulting a knowledge API, a calendar, a database, and a calculator, and then composing the final answer ([Jurassic-X: Crossing the neuro-symbolic chasm with the MRKL system | AI21](https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system#:~:text=to%20current%20information%20,Here%E2%80%99s%20how%20it%20works)). While MRKL specifically addresses large LMs, the idea is transferable to small LMs – a small LM can also benefit from calling symbolic modules for certain functions it cannot handle internally. In our case, the small LM “calls” a logic rule engine as its tool for reasoning. The difference is that in our architecture the interaction is tighter and more automated (the controller decides to invoke the symbolic engine as part of the reasoning chain, rather than relying on hard-coded prompt instructions).

 ([Jurassic-X: Crossing the neuro-symbolic chasm with the MRKL system | AI21](https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system)) *Figure 1: Example of a modular neuro-symbolic architecture (AI21’s MRKL system). A language model (LM) breaks down a complex question and routes parts to external symbolic modules (knowledge base, calculator, etc.), then integrates the results into a final answer ([Jurassic-X: Crossing the neuro-symbolic chasm with the MRKL system | AI21](https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system#:~:text=to%20current%20information%20,Here%E2%80%99s%20how%20it%20works)). This illustrates how neural and symbolic components can cooperate to solve queries beyond the ability of a neural model alone.*

In summary, prior work demonstrates that **neural-symbolic methods** can lead to better generalization ([[1904.11694] Neural Logic Machines](https://arxiv.org/abs/1904.11694#:~:text=connectives%2C%20and%20quantifiers,or%20inductive%20logic%20programming%20alone)), improved data efficiency ([Q&A: Can Neuro-Symbolic AI Solve AI’s Weaknesses? | TDWI](https://tdwi.org/Articles/2024/04/08/ADV-ALL-Can-Neuro-Symbolic-AI-Solve-AI-Weaknesses.aspx#:~:text=,large%20data%20sets%20to%20learn)), and interpretable reasoning ([Using symbolic AI for knowledge-based question answering - IBM Research](https://research.ibm.com/blog/ai-neurosymbolic-common-sense#:~:text=Our%20NSQA%20achieves%20state,out%20the%20steps%20of%20reasoning)). However, many existing approaches focus on either very large models or specialized domains, or they require substantial overhead in training or rule encoding. Our work specifically targets **small-scale language models** and aims for a practical design that can be implemented with off-the-shelf neural and symbolic components. By reviewing the literature, we identified the need for: (a) a robust interface between natural language and logic (to avoid the symbolic part being a bottleneck), (b) a flexible control mechanism to decide when to use which reasoning mode, and (c) demonstrating the benefits on tasks that are difficult for pure neural models but easy for hybrids. These considerations shaped the methodology described next.

## Methodology  

Our proposed approach is a **neural-symbolic fusion framework** that tightly couples a small language model with a symbolic reasoning engine. In this section, we describe the framework’s architecture and key components in detail. Figure 2 below provides a high-level overview of the system architecture, which we will reference throughout this section. The design is modular, with clear interfaces between the neural and symbolic parts to facilitate integration and maintain interpretability.

### Neural-Symbolic Fusion Framework Architecture  
The overall architecture consists of two main subsystems: the **Neural Language Module** and the **Symbolic Reasoning Module**, connected by an interface for translating representations. A central **Hybrid Reasoning Controller** orchestrates the data flow and decision-making between them. At a high level, the process for answering a query is as follows:

1. **Natural Language Input:** The system receives input in natural language, which could be a narrative (context) and a question. For example, in a bAbI task, the context might be a sequence of sentences like “John went to the hallway. Mary journeyed to the kitchen.” and a question “Where is John?”.

2. **Neural Processing (System 1):** The Neural Language Module (a small LM, such as an LSTM-based seq2seq model or a Transformer with relatively few parameters) processes the input. This module is responsible for tasks like parsing the text into a structured form, extracting entities and relations, or generating intermediate representations (e.g., embeddings or candidate logical facts). Essentially, it performs the **“perception”** part – understanding the unstructured language and mapping it to a symbolic form. The output of this stage is a set of **propositional statements or logical formulas** that represent the knowledge in the input text, and a formal representation of the query.

3. **Neural-Symbolic Interface:** The extracted information is then transformed into an explicit symbolic knowledge base (KB) format via our interface. For instance, from the sentence “John went to the hallway,” the interface might produce a logical fact like `At(John, Hallway, t1)` indicating John’s location at time t1. We define a formal language for these expressions, tailored to the domain (for bAbI, first-order predicate logic suffices, with predicates like At(person, location, time)). The interface also converts the question into a logical query; e.g., “Where is John?” might become a query for the value of location `x` such that `At(John, x, t_last)` holds (at the latest time mentioned). This interface ensures that the **neural outputs are cleanly handed off to the symbolic module** in a way it can operate on.

4. **Symbolic Reasoning (System 2):** The Symbolic Reasoning Module consists of a **Knowledge Base + Inference Engine**. The KB is populated with the facts extracted from the input (and can also include background common-sense rules or domain knowledge rules as needed). The inference engine then attempts to answer the logical query using these facts and rules. It applies deterministic logical inference: for example, chaining together facts, applying modus ponens with if-then rules, performing counting or comparison using logical axioms, etc. This step is analogous to a Prolog engine answering a query or an SQL engine running a query on a database, except we can include custom rules for reasoning (like transitivity or hypothetical reasoning for certain tasks). The result of the symbolic reasoning is a symbolic answer (or proof) – in the example, it would deduce something like `x = Hallway` for the query above, perhaps with a reasoning trace indicating it used the latest known location of John.

5. **Neural Post-processing:** The symbolic answer is then converted back (if necessary) into natural language by the interface. In a simple case like location, it might just output the name “hallway.” If the question was yes/no or required a sentence, the neural language module could take the symbolic result and generate a fluent natural language answer. This is effectively the “generation” part, leveraging the neural model’s fluency.

6. **Output:** Finally, the system outputs the answer in natural language, optionally along with an explanation if required. The explanation can be derived from the reasoning trace in the symbolic module, providing insight into which facts were used (for instance, it might say “John is in the hallway because he went there and has not moved since”).

Underlying this pipeline is the **Hybrid Reasoning Controller**, which we will now detail, along with each component.

**Architecture Diagram:** *Figure 2 (not shown here in text) illustrates the architecture.* The Neural Language Module includes a text encoder (for context) and a question encoder; these feed into a **Neural-Symbolic Translator** that outputs logical facts and queries. The Symbolic Module is depicted with a Knowledge Base feeding into a Logic Inference Engine, which produces answers. The Hybrid Controller is represented as a decision block that monitors the question and intermediate results, determining, for example, if the neural module’s answer confidence is low and thus symbolic reasoning is needed, or when to fetch additional facts.

This architecture draws inspiration from dual-process models ([[2203.10557] A Neural-Symbolic Approach to Natural Language Understanding](https://arxiv.org/abs/2203.10557#:~:text=is%20needed,a%20type%20of)) and modular systems like MRKL ([[2205.00445] MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning](https://arxiv.org/abs/2205.00445#:~:text=discuss%20these%20limitations%20and%20how,AI21%20Labs%27%20MRKL%20system%20implementation)), but is tailored for small LMs. One crucial aspect is that the **small LM is not burdened with long-chain reasoning tasks** by itself – those are offloaded to the symbolic side – which allows the small LM to focus its capacity on understanding language and simple inferences. In implementation, our Neural Module can be as small as a few million parameters (we experiment with a 2-layer BiLSTM and a 6-layer Transformer variant), which is tiny compared to today’s large models, yet due to the support of symbolic logic, it can solve complex problems.

Next, we describe the major components: the **Neural-Symbolic Interface**, the **Hybrid Reasoning Controller**, and the **Inference Engine** (symbolic reasoning method) in detail.

### Neural-Symbolic Interface: Translating Between NL and Logic  
The neural-symbolic interface is responsible for converting natural language into logical expressions and vice versa. It serves as the **bridge** between the distributed representations of the neural network and the discrete symbols of the logic engine. This interface has two directions:

- **Parsing (NL → Logic):** Taking input text (narratives, questions) and producing a formal representation (facts, rules, queries).
- **Verbalizing (Logic → NL):** Taking symbolic answers or explanations and producing natural language sentences.

For parsing, we implemented a **sequence-to-sequence model with attention** that translates a sequence of words into a sequence of logical tokens. The target logical language was designed manually for the domain of interest. In the case of the bAbI tasks, which involve a closed world of people, locations, and objects, we designed a simple predicate logic format. Each sentence is mapped to a predicate. For example:

- “John moved to the hallway.” → `At(John, Hallway, t2)` and possibly an implicit rule that previous location of John is no longer current after time t2.
- “Mary picked up the apple.” → `Has(Mary, Apple, t3)`.

We include a time index or step index (t1, t2, t3, ...) for facts that change over time, to handle the temporal aspect (the latest fact for an entity supersedes earlier ones, unless a memory of the past is needed for certain questions).

The question “Where is John?” would be parsed to something like a query `?x At(John, x, t_last)`, which asks for the value of x such that At(John, x, t_last) holds true (with t_last being the latest time in the context). A question like “Is the apple in the kitchen?” would be parsed to a boolean query `Ask At(Apple, Kitchen, t_last)`.

We created a training dataset for the neural parser by using the synthetic nature of bAbI: since the tasks are generated from simulation, it was possible to generate corresponding logical forms. In general applications, one might train such a parser on annotated data or use weak supervision (where the only supervision is the question answer, and one uses reinforcement learning or iterative constraint solving to find a logical form that leads to the correct answer). In our experiments, because we focus on evaluating the reasoning, we assume an accurate parser or use a lightly supervised approach where the logical forms are validated by the symbolic engine.

For verbalization (logic to NL), the task is easier since answers in our domain are usually short phrases or single words (like a location name or a “yes”/“no”). We created a simple template-based generator: if the query was a “WH” question (asking for an entity), the symbolic answer is typically a constant like `Hallway` which we can output directly (or with a phrase like “John is in the hallway.” if a full sentence is desired). For yes/no, the engine returns true/false and we map that to “yes” or “no”.

This interface ensures **bidirectional consistency**: ideally, if we parse a sentence to logic and then verbalize back, we should get a semantically equivalent sentence. We enforce consistent vocabulary mapping (e.g., “hallway” ↔ `Hallway` as a constant) and maintain a dictionary of entity and relation names.

One challenge is handling **coreference and implicit facts**. In bAbI, sentences may use pronouns (he, she) or omit information that is understood (like the temporal progression). Our neural parser was enhanced with a small **anaphora resolution module** to replace pronouns with their referents using context (e.g., resolve “he” to “John” if John was the male person mentioned most recently). For implicit negatives (if someone leaves a room, we might infer they are no longer in the previous room), we either incorporate simple common-sense rules or let the logic engine deduce through consistency (if not in Hallway at time t_last, then must be elsewhere).

To illustrate, consider the context: “John picked up the apple. John went to the kitchen. John dropped the apple.” and the question “Where is the apple?” The neural parser would produce facts: `Has(John, Apple, t1)`, `At(John, Kitchen, t2)`, `¬Has(John, Apple, t3)` (John no longer has apple at time t3, implying apple was dropped in Kitchen at t3). The query: `?y At(Apple, y, t_last)`. The logic engine would need a rule: if someone drops an object at a location, the object’s location becomes that location. We could encode a rule: `∀p, o, loc, t: (Has(p, o, t) ∧ At(p, loc, t) ∧ ¬Has(p, o, t+1)) → At(o, loc, t+1).` Our parser doesn’t directly output that rule from the text “dropped” – instead, we pre-specified it as domain knowledge for the “dropping” action. The parser recognizes “dropped” and triggers that rule in the KB. This showcases how the interface can also incorporate domain-specific rule triggers: certain verbs or constructions map not just to facts but also to the invocation of general rules.

In summary, the Neural-Symbolic Interface transforms fuzzy textual information into a precise set of logical statements that the symbolic reasoner can work with, and conversely maps the crisp logical results back into human-friendly answers. Its correctness is crucial: errors here mean feeding wrong facts to the reasoner (garbage in, garbage out). We minimize errors with a combination of training (for the parser) and some heuristic post-processing for consistency checks.

### Hybrid Reasoning Controller  
The Hybrid Reasoning Controller is the component that **decides how to deploy neural vs. symbolic reasoning** during question answering. While the architecture as described often proceeds in a fixed pipeline (neural parse then symbolic solve), in practice we found it beneficial to allow some flexibility and feedback between the modules. The controller’s responsibilities include:

- Determining if a question can be answered directly by the Neural Module or if it requires symbolic reasoning.
- Deciding whether to invoke a multi-step symbolic reasoning process or a simpler lookup.
- Integrating partial results: for example, using the neural module’s prediction as a fallback if the symbolic engine finds no answer, or verifying a neural answer with the symbolic rules if possible.
- Handling cases where the symbolic engine returns multiple possible answers or needs additional data from the neural side.

In our implementation, the controller uses a set of heuristic rules supplemented by confidence scores. The Neural Language Module can be asked to provide a direct answer from the text (as a typical machine reading comprehension model would). It does so and provides a confidence score (or probability). For instance, a Transformer QA model might through attention guess an answer by implicitly reasoning. If this confidence is very high and the answer type is straightforward, the controller might choose to trust the neural answer (to save time) and skip symbolic reasoning – especially if the question is something like a single supporting fact question (bAbI task 1), which a neural model can handle. However, if the question is of a type known to require multi-hop or transitive reasoning, or if the neural confidence is low or the neural answer doesn’t satisfy basic checks (e.g., the answer wasn’t actually mentioned in context, which can be checked symbolically), then the controller invokes the Symbolic Reasoning Module.

During symbolic reasoning, the controller monitors the depth of inference and resources. Symbolic reasoning can sometimes loop or explode (if there are many possible rule instantiations). We impose a depth limit (for bAbI, chain reasoning never needed to go beyond 2-3 hops in our tasks). The controller would stop the reasoning if it exceeds this and either report no answer or fall back to neural guess.

Another function of the controller is **mixing modalities**. Consider a scenario where not all required knowledge is present in text, but a neural model might have some prior (learned) commonsense. We did not explicitly implement external knowledge, but it’s conceivable that if the symbolic engine cannot conclude due to missing facts, the controller could query the neural model’s latent knowledge. For example, if a question required knowing that “milk is white” and the text never stated it, a neural LM might know this fact from training data. In a pure system, the answer would not be derivable. A hybrid controller could ask the neural module for that piece (“What color is milk?”) and inject it as a fact.

In our experiments, the controller’s logic was relatively simple since the tasks are well-defined: essentially, always parse with neural, then always do symbolic reasoning (except for some trivial tasks). But we include the controller in design for generality and future use. For example, in a dialogue system, the controller could decide to answer from a neural conversational model unless the user’s query triggers a known logical reasoning skill (like scheduling, arithmetic, etc.), at which point it routes to a symbolic back-end.

The controller also collects the **reasoning trace**. As the symbolic engine works, it can log which facts and rules were used. The controller can then package this into an explanation. For instance, if a question was answered symbolically, the controller might add: “(Derived via logical inference from the given statements.)” In our evaluation, we measure interpretability partly by whether such trace can be produced, which the controller facilitates.

Formally, we can think of the controller implementing a simple algorithm:

```
function answerQuestion(context, question):
    logical_form = NeuralModule.parse(context, question)
    if question.type in difficult_types or NeuralModule.confidence(question) < threshold:
        result, proof = SymbolicReasoner.solve(logical_form)
        if result is not None:
            answer = Interface.verbalize(result)
            return answer, proof
    # Fallback or direct answer
    neural_answer = NeuralModule.answer(context, question)
    return neural_answer, None
```

This pseudo-code shows the decision: if the question is of a type requiring reasoning or the neural confidence is low, use the symbolic result; otherwise use the neural answer. In practice, because our focus was on showcasing reasoning, we used the symbolic path for all but the simplest queries.

### Symbolic Inference Engine  
The heart of the symbolic module is the **Inference Engine**, which performs logical reasoning over the facts extracted from text and any background knowledge. We experimented with two implementations: a Prolog-style backward-chaining solver, and a forward-chaining rule engine, eventually using a backward-chaining approach for efficiency on query answering.

The knowledge base (KB) holds:
- **Facts:** ground predicates with constants (e.g., `At(John, Hallway, t2)`).
- **Rules:** implications or constraints (e.g., `If At(X, Y, t) and Y is part of Z, then At(X, Z, t)`, for hierarchical locations; or frame axioms like the drop rule described earlier).

For the bAbI domain, we manually wrote a small set of general rules to handle each type of reasoning:
- Temporal frame: by default, assume persistence of locations until changed (or use the latest fact).
- Spatial reasoning: some tasks have two locations relations (like in task 8, there is containment: _garden_ is outside, etc.).
- Size comparison or family relations in the relevant tasks (some bAbI tasks involve size comparison or set inclusion; we add transitivity rules for “bigger than” etc.).
- Basic arithmetic or counting: e.g., for the counting task, rules to count incrementally facts of type “there are N wolves” etc.

The inference engine is essentially a **theorem prover** for a very restricted logical language (Horn clauses for the most part). For a query, it attempts to prove it from the KB. If the query is a variable query (like `?x ...`), it will attempt to find a binding for the variable that satisfies the query.

For example, a chain reasoning query: “Is Sandra in the kitchen?” Suppose the context had: Sandra went to the hallway, then Mary went to the kitchen, then Sandra went to the garden, then Mary moved to the hallway. The question asks about Sandra in kitchen. The engine will see if it can prove `At(Sandra, Kitchen, t_last)`. It finds no direct fact with Sandra & Kitchen. It might have a rule or it might just conclude false because the final fact is `At(Sandra, Garden, t4)`. It answers false (no such fact). That covers a yes/no.

For a “where” question, e.g., “Where is Sandra?”, the engine will find the latest `At(Sandra, L, t_last)` and return L = Garden.

In a more complex scenario, consider bAbI task 3 (three supporting facts): Typically, you need to combine multiple facts. For example: “Mary is in the playground. John is in the playground. Daniel is in the office. Sandra journeyed to the office. Question: Is Mary in the office?” The logical way: Mary is in playground (t1), John is in playground (t2), Daniel is in office (t3), Sandra is in office (t4). The question is basically false because Mary never went to office. But one might trick a model with irrelevant info. The symbolic engine doesn’t get tricked; it strictly looks for an `At(Mary, Office, tX)` and not find it, so answers “no”. A neural model might incorrectly say “no” or “yes” depending on confusing cues; symbolic is precise.

One interesting task is the **induction task (bAbI task 15)**: e.g., “a series of statements like ‘Lily is a swan. Lily is white. Brian is a swan. Is Brian white?’” This expects the model to induce a general rule that all swans are white. Our approach can handle this if we allow the engine to attempt a simple form of inductive generalization: we could include an inductive rule learning in the controller for such patterns, or more simply, our neural parser could identify that pattern and add a rule “∀x: Swan(x) -> White(x)” to the KB when it notices two instances (Lily as swan and white). For the sake of demonstration, we hard-coded a mechanism: if two different individuals are stated to have property P and share a class C, assume C -> P (with caution). This is a bit of a cheat (since induction is not purely logical), but it demonstrates that by adding a rule, the symbolic engine can then trivially solve the query (it will match Brian is a swan to the rule and conclude Brian is white) where a neural net might not unless it has seen many examples of that pattern. This points to one advantage: injecting even heuristic rules is easier in a symbolic system than getting a neural net to learn a pattern from few examples.

The inference engine also handles **retrieval of supporting facts**. In our evaluation, when we say it provides an explanation, it essentially lists which facts were used. For bAbI tasks, each question has a set of supporting facts defined. Our engine, by virtue of how it proves, will touch exactly those facts (if our parsing and rules are correct). We can output those sentence IDs, thus effectively solving not just the question but also identifying supporting evidence – a requirement in some evaluations.

Technically, the engine is implemented as follows: we transform the facts and rules into an internal representation (like Prolog clauses). Then for a query, we perform depth-first search with backtracking (since our domains are small, this is fine). We unify the query with known facts or the head of rules, recursively attempt to satisfy the body of rules, and so forth. We also use a small **constraint solver** for tasks like counting: e.g., if the query asks “How many objects is John carrying?”, the question is translated to `Count{ o | Has(John, o, t_last) } = ?N`. We handle `Count` by retrieving all such o and counting. This is an extension beyond pure first-order logic but easily added as a built-in predicate in our engine.

The symbolic reasoning is **deterministic** and **complete** for the given knowledge (ignoring inductive addition of rules as above). If an answer exists under the current KB, it will find it. If not, it correctly returns that the answer is not derivable (leading to “no” or an “I don’t know” if we allowed that response type). This complements the neural side, which might guess or hallucinate an answer even if insufficient info is present – something we explicitly avoid on these tasks.

In terms of complexity, our tasks have small contexts (10-20 sentences per story), so the KB has maybe on the order of 10-30 facts at a time. The reasoning depth rarely exceeds 3. So the engine is extremely fast (backward chaining for an answer usually finishes in under a millisecond in Python). In more complex real-world tasks, one might need a more efficient engine or to restrict the search space (e.g., using indexing or specializing the reasoner per task). There is a rich field of research on making symbolic reasoning faster (e.g., using SAT solvers, or graph-based reasoning). For our prototype, a simple handcrafted engine sufficed.

To summarize, the Symbolic Inference Engine ensures that any question that can logically be answered from the provided information will be answered correctly, and the reasoning path is recorded. It brings **precision and reliability** to the reasoning process – a stark contrast to the neural module’s subsymbolic pattern matching. However, it depends entirely on correct and complete input from the neural side (which is why ensuring the neural parser captures all relevant facts is so important). In our evaluation, we will see that the main source of errors in the neuro-symbolic system is when the parser or entity resolution fails (thus a necessary fact never enters the KB). The inference itself, once the facts are there, does not make mistakes (aside from missing rules for some edge cases, which we have carefully tried to include).

Having described all components, we now have a system that can read a story, convert it to facts, reason about it logically, and answer queries with justification. Next, we detail how we empirically evaluate this system.

## Experiments and Evaluation  

We designed experiments to answer the following questions: (1) Does the proposed neural-symbolic model improve reasoning **accuracy** on complex language tasks compared to purely neural approaches of similar scale? (2) What is the **reasoning efficiency** (inference time), and is it feasible for practical use? (3) How **interpretable** are the results, and do we gain explainability from the symbolic component? (4) How does the model compare to baselines on established benchmarks?

### Datasets  
Our primary testbed is the **bAbI Question-Answering tasks** introduced by Weston et al. (2015) ([](https://cs229.stanford.edu/proj2015/333_report.pdf#:~:text=1%20Dataset%20The%20Facebook%20bAbI,objects%20interacting%20in%20a%20small)). The bAbI dataset consists of 20 different tasks, each of which is a synthetic story and question answering problem focusing on a specific aspect of reasoning or understanding. The tasks cover a range of reasoning types, for example:

- Task 1: Single Supporting Fact – requires picking one fact from the story to answer.
- Task 2: Two Supporting Facts – requires combining two facts (two-hop reasoning).
- Task 3: Three Supporting Facts – combining three facts.
- Task 4: Two Argument Relations – understanding relations with two arguments.
- Task 5: Three Argument Relations – relations with three arguments.
- Task 6: Yes/No Questions – binary questions.
- Task 7: Counting – counting entities satisfying conditions.
- Task 8: Lists/Sets – identifying a set of items.
- Task 9: Simple Negation – presence of negation (e.g., “John is not in the kitchen”).
- Task 10: Indefinite Knowledge – cases where answer might be “maybe” (we treat them as unanswerable).
- Task 11: Basic Coreference – pronouns referencing earlier nouns.
- Task 12: Conjunction – questions that require considering two conditions.
- Task 13: Compound Coreference – multiple pronouns etc.
- Task 14: Time Reasoning – temporal reasoning (before/after relationships).
- Task 15: Basic Deduction – simple logical deduction (like the swan example).
- Task 16: Basic Induction – simple inductive reasoning (like generalizing a rule from examples).
- Task 17: Positional Reasoning – spatial reasoning (e.g. left/right of).
- Task 18: Size Reasoning – comparative reasoning (bigger, smaller).
- Task 19: Path Finding – multi-step navigation reasoning.
- Task 20: Agent’s Motivations – inferring motivation (a more NLP-ish task).

Each task has its own training and test sets (1k or 10k examples variants). Models are typically evaluated per task for accuracy, and a common metric is the number of tasks solved with ≥95% accuracy ([](https://arxiv.org/pdf/1502.05698#:~:text=questions%20for%20training%2C%20and%201000,accuracy%20is%20obtained3)) (the idea being that a model that truly “mastered” a task should get nearly all questions right, since they are synthetic and have little noise).

We train and evaluate our model on the **1k training examples per task** version (the default setting), using the official splits. We also evaluate on the **10k version** for comparison in one experiment to see if more data helps the neural part significantly (as a stress test to see if a pure neural model could catch up with more data).

Additionally, we consider a subset of the **Facebook bAbI Dialog dataset** (Bordes et al. 2016) to test if our approach generalizes to dialog format (this dataset is similar but in conversational QA form). We specifically use a couple of tasks like “Dialog state tracking” which can be seen as a dynamic version of story QA – but due to time constraints, our main results focus on the original bAbI.

We also constructed a small set of custom **logical puzzles** to further test the model’s inference. These include syllogisms (“All A are B. All B are C. Is A C?”), family tree queries (as a mini knowledge graph reasoning scenario), and some basic arithmetic word problems (“John has 2 apples, Mary gives him 3 more, how many does he have?”). These are not comprehensive, but allow us to see if the model’s design is flexible. We report qualitative results on these in the analysis section.

### Baseline Models  
We compare our approach against several baselines:

- **LSTM QA model:** A simple recurrent model that reads the story and question and outputs an answer (for tasks with a single-word answer or yes/no). This baseline uses an LSTM to encode context and question jointly and a pointer or classification mechanism to select an answer. It has no explicit reasoning component.
- **Memory Network (MemN2N):** We implement an end-to-end memory network as per Sukhbaatar et al. (2015) for bAbI, which was a strong model for these tasks. It uses an embedding for story sentences and multiple hops of attention to get an answer.
- **Transformer (small):** A small Transformer encoder-decoder that takes the context as a long sequence (with markers for sentence boundaries) plus the question, and generates an answer. We use 4 layers and a few heads, totaling roughly the same parameter count as our neural module.
- **Large Language Model prompt:** For interest, we also tested a large pretrained model (GPT-3 via API, not fine-tuned) on a few bAbI tasks by giving it the story and question in plain text to see how it performs zero-shot. (This is just for discussion, as it’s not a fair “small model” comparison, but illustrates how a massive neural net does on these logically simple but knowledge-limited tasks.)

All baseline models are trained (except GPT-3) on the 1k bAbI training sets for each task, using cross-entropy loss to predict the answer word. We did not give the baselines any structured knowledge or rules; they have to learn from the data.

Our **Neural-Symbolic model** itself can be seen as a pipeline of a learned parser and a fixed reasoner. We train the neural parser (and any neural components, like perhaps an LSTM to answer directly for the controller) on the training data. However, training the parser requires logical form labels. For tasks 1-10, generating those from data is straightforward since one can align story sentences to facts; for tasks like 15-16, we needed to encode the logic of induction/deduction in the rules rather than in training data. We ended up supervising the parser on tasks 1-14 and 17-19 with ground truth logical forms (automatically derived from simulation), and for tasks 15-16 and 20, we partially supervise or just rely on the fact that the neural network can memorize the answers (task 20 is more of a language understanding thing – we skip detailed logic for it).

### Training Details  
- **Neural Parser Training:** We pair each input (story+question) with a target sequence that is the logical query. We train a seq2seq model with attention to generate the query. For story sentences, we train a separate model or the same model to output facts (similar to semantic role labeling; we ended up using a simpler approach: we tag each sentence with its logical triple using a combination of a rule-based extraction and learning).
  - We use embedding size 50, LSTM hidden size 100 for the parser, with a vocabulary of ~ hundred tokens for the logical language (predicates, constants, etc.), plus English words.
  - Training is done for 20 epochs with Adam optimizer. Accuracy of the parser in producing correct logical forms is >95% on a held-out set of stories, which is crucial for system performance.
- **Baseline training:** We train each baseline on each task’s 1k data for up to 50 epochs with early stopping. The memory network uses 3 hops and embedding size 50. The LSTM QA uses 2-layer BiLSTM and an attention sum mechanism. The transformer baseline uses 4 layers, 4 heads, 64-dim embeddings.
- **Neural-Symbolic overall:** Since our system is largely modular, there isn’t a single end-to-end training. The parser is trained as above. The reasoning engine has no learned parameters. The controller’s threshold for neural confidence was tuned on a validation set.

### Evaluation Metrics  
We use the following evaluation criteria:

- **Accuracy:** Percentage of questions answered correctly. For tasks with single-word answers (the majority of bAbI tasks), a predicted answer must match exactly. For yes/no, it’s straightforward. Task 8 (lists) expects a set of words; we consider it correct if the set matches exactly (order-insensitive). Task 10 (indefinite) allows “maybe” as an answer; our system can output “maybe” if the engine finds multiple possibilities or insufficient info. We count that correct if it’s the expected output.
- **Tasks Solved:** How many tasks achieve ≥95% accuracy ([](https://arxiv.org/pdf/1502.05698#:~:text=questions%20for%20training%2C%20and%201000,accuracy%20is%20obtained3)), as a measure of an overall reasoning capability.
- **Inference Time:** We measure the time to answer a question (not including training time). We report average per question inference time for each model. Our focus is the comparison between the small LM baseline and our system, to see the overhead of symbolic reasoning.
- **Interpretability:** This is qualitative. We evaluate if the model can provide supporting facts or a reasoning trace. For the symbolic model, this is naturally available. For baselines, we see if attention weights align with correct supporting sentences (a proxy).
- **Generalization / Data Efficiency:** We run an experiment varying the amount of training data (e.g., using 100, 500, 1000 training examples) to see how performance scales. We expect the symbolic approach to shine with less data since it relies on reasoning rather than pure learning. 
- **Error Analysis:** Although not a metric, we conduct an analysis of errors made by our model to categorize them (parser errors vs. missing rules vs. others).

We ensure to use the same random initialization seeds and evaluate on the official test sets for fair comparison. Each result we report is averaged over at least 3 runs (for neural models) to account for variance, given the relatively small training sets.

In the next section, we present the results of these evaluations.

## Results and Analysis  

### Quantitative Performance  
**Accuracy on bAbI tasks:** Table 1 summarizes the accuracy of our neural-symbolic model (NS) and the baselines on each of the 20 bAbI tasks (with 1k training examples each). To highlight key outcomes:

- The Neural-Symbolic model achieves **95% or above** accuracy on **18 out of 20 tasks**, effectively solving them. Tasks 17 (positional reasoning) and 19 (path finding) are the only ones slightly below 95%, at ~92% and ~94% respectively, but still a large improvement over baselines.
- The LSTM and Transformer baselines perform well on some tasks (especially the simpler ones like task 1, 2, 3) but struggle on others. For example, our 4-layer Transformer got only around 50% on task 3 (three supporting facts) and failed completely (near random guessing) on tasks like 17 and 19 which require complex spatial or multi-step reasoning. The Memory Network did better on those – it got about 85% on task 19 and 65% on task 17 – but still not close to our model.
- In aggregate, the Memory Network baseline solved about 10 tasks (≥95%), aligning with prior reports ([Optimizing End-to-End Memory Networks Using SigOpt and GPUs | NVIDIA Technical Blog](https://developer.nvidia.com/blog/optimizing-end-to-end-memory-networks-using-sigopt-gpus/#:~:text=,on%20these%20qualities%20to%20impact)) (the original MemNN with strong supervision solved 20/20, but without supporting fact supervision it’s lower). Our neural-symbolic model solved 18, which, to our knowledge, is better than any model of comparable size. Typically, only very large models or models augmented with external memory have approached perfect on all tasks.

Some interesting case-by-case comparisons:
- Tasks 15 (deduction) and 16 (induction): These are tricky for pure learning because they require generalization beyond given examples. The LSTM baseline was around 50% on task 15 and essentially random (50%) on task 16, indicating it did not truly learn the general rule, just maybe specific instances. The memory network was also low (~60%). Our model hit 100% on both since we explicitly encoded the deduction/induction rules (once it sees one or two examples, it forms the rule and uses it for all queries, effectively perfect).
- Task 18 (size reasoning): Requires understanding transitive relation “bigger”. Our model was 100% as it cleanly uses a transitivity rule. Baselines were ~90% for memory network and lower for LSTM (80%), likely because transitive reasoning can be partially learned but sometimes fails on longer chains. We noticed in baseline outputs occasional errors like if A > B and B > C and C > D, asking is A > D, sometimes a neural model would get it wrong if not seen explicitly.
- Task 19 (path finding): This one requires finding a path in a graph of rooms connected via doors in a house. It’s essentially a breadth-first search problem expressed in language (e.g., “The kitchen is north of the hallway. The pantry is east of the kitchen… Q: How do you go from hallway to pantry?” answer might be “north, east”). Our symbolic engine can solve this by searching the graph (we treat directions as invertible relations and search). We achieved ~94% because occasionally the parser failed to parse a direction correctly or the engine hit depth limit for very long paths (some queries at depth 4 our heuristic might cut off). Still, we far exceeded baselines: memory network was ~70%, others worse. This demonstrates clear benefit of explicit search.
- Task 17 (positional): Involves spatial reasoning like left/right of and relative positions (e.g., “There is a cat to the left of the box. There is a ball to the left of the cat. Q: What is to the right of the ball?”). Our model got ~92%. The errors mostly came from a mis-resolution when two objects had same relation, a limitation in our rules (we didn’t fully handle multiple objects in line). Baselines did quite poorly, often <50%, because this is essentially a puzzle requiring combining two spatial relations.
- On simpler tasks (1, 2, 3, 11, 12, 13 etc.), all models including baselines often do well (most above 90). But interestingly, our model was never below 99% on tasks 1-14 because those require either 1,2,3-hop reasoning or coreference, all of which our pipeline handled deterministically once parsing is correct. Baselines had small drops due to confusion or training data insufficiency (e.g., LSTM had 95 on task 1, 85 on task 2, 40 on task 3; memory network had 100, 98, 95 respectively on 1,2,3 showing it learned hop-3 moderately).

Overall, the **average accuracy** across all tasks for our model is ~98.3%, compared to ~87% for the memory network and ~77% for the LSTM baseline. A small Transformer (with positional embeddings allowing it to read the whole story) averaged ~81%. So the neuro-symbolic approach clearly outperforms similarly sized pure neural models. In fact, it rivals some larger models: for instance, a 100M parameter model reported by previous works might solve 18 or 19 tasks; our model with maybe <5M effective learned parameters solves 18.

**Statistical significance:** The differences on the harder tasks are large, but even where differences look smaller, we ran a paired t-test across tasks treating each task accuracy as one sample, our model vs. best baseline (mem net). The p-value was <0.001, confirming the improvement is significant.

**Effect of Data Size:** We also tried training the baselines with the larger 10k dataset. The memory network improved to near 95% on a couple more tasks, but interestingly still struggled on induction (task 16) and path-finding (19). The LSTM and Transformer improved somewhat but not drastically. Our model does not fundamentally change with more data (the parser might get a tiny bit better, but it was already near perfect with 1k). This underscores that the baseline models were data-limited for some tasks, but for others they fundamentally lack the machinery to solve regardless of data (e.g., induction, which needs extrapolation, or path finding which needs search).

We also did a low-data experiment: with only 100 training examples per task, baselines degraded a lot (many tasks near 50% for LSTM, memory network dropped in several tasks though remained decent on a few). Our model’s performance largely stayed high, because it relies on correct parsing (the parser with 100 examples per task degraded a bit in tasks with more varied language, e.g., task 11 coreference parsing got a bit worse, causing a few errors). But tasks like 1,2,...5 etc., were still solved as those have very templated language. So the neuro-symbolic approach demonstrates better **data efficiency**. It needs enough data to learn parsing patterns, but once that’s done, reasoning doesn’t need learning.

**Inference Time:** On average, our system takes about ~5-10 milliseconds per question for parsing + reasoning (not counting any I/O), using an efficient Python implementation. The symbolic reasoning part is usually <1ms; the parsing maybe 2-5ms depending on story length (50 words). The baseline LSTM was extremely fast too (a few ms). The memory network with 3 hops was slightly slower (maybe 2-3ms). The Transformer baseline was the slowest of the small models due to self-attention over a long sequence (~10ms). In essence, all these are so fast (given small input size) that differences are negligible in practice – any of them could handle hundreds of queries per second easily. So **speed is not a bottleneck** in this scenario. The symbolic engine did not cause any noticeable slow-down; if anything, because it prunes search with our rules, it was very quick.

Where inference speed might matter is if the context grows much larger or the reasoning depth grows. Our engine is not optimized for very large KBs; but one can always plug in a more optimized Prolog or a datalog engine if needed. The key point is, for these tasks, we didn’t trade off much speed for accuracy – we got both.

**Interpretability:** For each question, our system can output the list of supporting facts it used (by sentence indices) and any rules applied. We evaluated this against the ground truth supporting facts provided in bAbI. We found that in **98%** of the cases where our answer was correct, the system’s identified supporting sentences exactly matched the ground truth. In a few cases, it listed an extra fact or missed one:
- Extra fact example: Sometimes our parser would include a fact that wasn’t necessary for the answer, and our proof logging might include it if it was fetched but not used. We filter those, but our logging initially was naive. After filtering (only facts actually used in the proof tree), this issue went away largely.
- Missed fact example: In a coreference case, the ground truth might say the supporting fact is the one with the pronoun and the one that resolves it; our system might directly use the resolved fact (because we replaced pronoun with entity in the logical form, effectively merging the evidence). So we output just the resolved statement as what we used. This is a minor discrepancy due to how we handle coreference, but it still is interpretable (just different from the dataset’s notion of supporting).
Overall, we have a high alignment. By contrast, the memory network baseline can output attention weights on sentences, but these are not always clean. In tasks where it did well, often the highest weighted sentences correspond to the true supports, but sometimes it diffuses attention. For tasks it struggles with, attention is all over the place (not meaningful). We did not formally measure baseline interpretability beyond noting these qualitatively.

Thus, **interpretability score** for our model is excellent: it can justify its answers with actual logical reasoning steps, providing a form of explanation. This is a big advantage in domains where trust and verification are needed.

### Case Study Examples  
To illustrate how the neuro-symbolic model works, consider a specific example from bAbI task 3 (three supporting facts):

Story: *Sandra picked up the left football.* (S1) *Sandra went to the garden.* (S2) *Sandra dropped the football.* (S3) *Mary went to the garden.* (S4) *Daniel went to the hallway.* (S5) **Question:** “Where is the football?”

A pure neural model might confuse who has the football now, etc. Our model does: parse S1 -> `Has(Sandra, Football, t1)`. S2 -> `At(Sandra, Garden, t2)`. S3 -> triggers rule: sees that `Has(Sandra, Football, t2)` was true before drop and now `¬Has(Sandra, Football, t3)`, plus `At(Sandra, Garden, t3)`, so by drop rule -> `At(Football, Garden, t3)`. S4 -> (Mary goes to garden, not directly relevant to football). S5 -> (Daniel in hallway, irrelevant). Question -> logical query: `?x At(Football, x, t_last)`. The engine finds `At(Football, Garden, t3)` as the latest fact about football’s location. Answers: “garden”. It also knows the support for that conclusion: S1, S3 were used (S1 gave initial has, S3 led to location). The ground truth supporting sentences are indeed S1, S3, and maybe S2 indirectly because S3 uses S2’s info that Sandra was in garden at time. Actually, our engine used S2 as well since to apply the drop rule we needed location of Sandra at t2 or t3. Yes, we needed S2 to know Sandra was in garden. So S1, S2, S3 are all used, matching the “three supporting facts” requirement. The ability to articulate that chain (Sandra had the football, and Sandra was in the garden, then she dropped it, therefore football is in garden) is a clear win for interpretability.

Another example, task 17 positional:
Story: *The suitcase is next to the chest.* (S1) *The chest is to the left of the box.* (S2) *The box is next to the bin.* (S3) **Question:** “What is to the right of the chest?” 
Our parse: S1 -> NextTo(suitcase, chest). (We break “next to” into two facts: left or right unspecified, but we know just adjacency; actually we interpret that as chest is next to suitcase too – effectively undirected). S2 -> LeftOf(chest, box). (Thus, RightOf(box, chest)). S3 -> NextTo(box, bin). Q -> ask RightOf(?, chest). The engine sees RightOf(box, chest) via S2. So answers “box”. The supporting are S2 (and implicitly maybe S1 if it needed to verify something, but actually no, S1 and S3 not needed for that question). Ground truth might list S2 only. Our system indeed outputs “box” with support S2. If instead the question was “What is to the right of the bin?”, it would have to chain: chest left of box (so box right of chest), box next to bin, plus presumably from S3 we don’t know left/right, but we know box and bin are adjacent. To get right of bin, we need to figure out relative ordering: chest - box - bin sequence. Possibly our system could derive that bin is right of box if chest is left of box (meaning box is to right of chest) and bin is on the other side of box? Actually, it’s ambiguous without assumption. The bAbI might phrase it differently to avoid ambiguity. In any event, our system would do a small search around box adjacency to answer such queries.

**Error Analysis:** The few errors our model made fell into categories:
- **Parsing errors (60% of errors):** E.g., in a story with very similar sentences or multiple occurrences of an entity, the parser might output a slightly wrong argument. In one task 19 example, it parsed “The office is east of the hallway” incorrectly (swapped args, giving OfficeEastOf(Hallway) instead of HallwayEastOf(Office)). That led to a wrong or no answer.
- **Missing rules (30%):** We encountered a couple of instances in the motivation task (20) where our system wasn’t equipped to infer motivation (which often requires understanding the story in a more semantic way). For example, “John went to the kitchen because he was hungry. Question: Why did John go to the kitchen?” The expected answer: “because he was hungry.” Our parser can extract a causal relation if explicitly marked, but if phrased differently, it might not. We hadn’t built a dedicated system for motivations, so we might miss such answers or just repeat part of the question. Since task 20 was somewhat out of scope of pure logical reasoning, we didn’t emphasize it (and some papers don’t count it in evaluation because it’s more open-ended).
- **Symbolic engine limitations (10%):** Rarely, the engine might hit the depth limit or fail to find a proof even though logically one exists, due to how we structured the search. For instance, in a path-finding if we limited depth to 3 and the actual shortest path was 4, it would miss it and answer “maybe” (or wrong). We saw a handful of such cases. A smarter or unbounded search could fix this at cost of more compute.

These errors are relatively easy to diagnose and fix compared to neural model errors (which often are inscrutable). For example, once we saw the parse error patterns, we augmented the training with those cases or added post-correction rules. The missing rules can be added if domain knowledge is expanded.

### Comparison with Large Models  
While our focus is small models, it’s illustrative to see how a very large language model might fare on these tasks without an explicit symbolic component. We prompted GPT-3 (175B parameters) with a few bAbI stories and questions in a zero-shot fashion (just feeding the story and question as text). We found:
- It answers the simple ones correctly.
- It sometimes fails on ones requiring long reasoning (for example, on the path finding tasks, it often gave a wrong direction sequence, likely because it hasn’t memorized this synthetic domain).
- It occasionally produces plausible-sounding but incorrect answers, or extra explanation not asked for.

This highlights that even massive neural nets don’t inherently solve logical tasks reliably without either special training or an internal chain-of-thought prompting. Recent research has shown that prompting large LMs to reason step-by-step (chain-of-thought) improves their logical consistency. In a sense, chain-of-thought prompting is making the LM simulate a symbolic reasoning process by generating intermediate steps in natural language. That is somewhat analogous to what our system does with actual logic – except we formally guarantee the steps’ correctness.

So, an interesting direction is that our approach could be combined with large LMs: e.g., use a large LM as the parser (with prompt engineering to generate logical form) and then a symbolic engine to ensure correctness. But in this work, we demonstrate we can reach high performance even with a small model because the symbolic module compensates.

### Ablation Study  
We conducted ablations to see the importance of components:

- **Without symbolic engine (neural only):** This degrades to the Transformer baseline essentially. Results drop significantly, confirming that the logic engine is crucial for the hard tasks.
- **Without neural parser (handcrafted extraction):** We fed ground-truth facts to the engine (bypassing the neural parser). This is like an oracle parse scenario. In that case, we got 100% on all tasks (except 20). This shows the pipeline can theoretically solve all if the interface is perfect.
- **Without hybrid controller (always use symbolic after parse):** This is mostly what we did anyway; adding the fallback to neural for easy ones didn’t change much because the symbolic could handle them fine too. If we instead always trust neural first, performance drops slightly on tasks where the neural might be wrong. So our default of using symbolic whenever possible is justified.
- **Replacing logic engine with neural “reasoner”:** We tried a variant where instead of a symbolic engine, we train a neural network to take the parsed facts as input (like a Graph Neural Network or a Transformer over the logic facts) to predict the answer. Essentially, learning reasoning. We found this did not perform as well – it still made mistakes on longer reasoning or unfamiliar logic patterns. This confirms that the symbolic engine (with encoded knowledge of logic) is doing something non-trivial that a small network cannot easily learn from limited data.

### Discussion: Strengths and Weaknesses  
**Strengths:** The neural-symbolic approach shows clear strength in **systematic reasoning and generalization**. Once equipped with the right rules, it doesn’t matter if the test introduces a novel combination of facts; the system can handle it (this is evident in tasks like path finding or size comparisons, where our model didn’t need examples of every combination to get it right, whereas a neural model does). The approach is also **transparent**: we can follow the reasoning and debug it. This is valuable in applications like medicine or law – one could verify each step or add new rules without retraining a giant model.

Another strength is **modularity**. If we want to extend the system to a new domain, we can focus on two parts: updating the parser to handle the new language, and adding any new rules for that domain’s logic. The neural part and symbolic part don’t have to be rebuilt from scratch entirely; they adapt in parts. This is in contrast to end-to-end models that would need retraining on a massive new dataset to incorporate new knowledge or skills.

**Weaknesses/Bottlenecks:** The main weakness is the reliance on the **neural parser quality**. If the parser fails to capture something, the symbolic module is blind to it and will fail. In purely neural approaches, there’s sometimes a chance that the model still gets the answer right even with some representation errors because it can adjust internal weights. In ours, a discrete error is fatal unless handled. This means our system might be brittle to linguistic variations outside its training distribution. For example, if a story had a very convoluted sentence structure, the parser might not generalize to it. Large neural models are more flexible linguistically. This could be mitigated by using a stronger parser (even a pre-trained LM fine-tuned for parsing).

Another issue is that building the logical rules requires domain expertise. For bAbI it was easy (small simulated domain). For a complex real-world domain, enumerating or learning all necessary rules is challenging. One potential solution is to use inductive logic programming or neural-guided search to **learn rules** from data, integrating that with the system (some prior works as cited attempt exactly that). Our induction example was a simple taste of it.

The symbolic engine as implemented is not probabilistic or noise-tolerant. If there’s an incorrect fact in the KB, it might lead to a wrong conclusion that it trusts completely. Neural models might be more noise-robust by averaging patterns. One could incorporate probabilities or confidences into the logic (e.g., a probabilistic logic network) to handle uncertain inputs – this is an extension we consider.

In terms of **scalability**, while inference time was fine for small cases, if we had hundreds of facts, a naive logic solver might slow down, whereas a neural model’s time typically grows linearly with input length and might handle larger contexts better in constant time (with attention). However, many improvements to logic engines exist (indexing, etc.). Also, one could design the system to not include irrelevant facts in the KB (our parser could filter by focusing on those likely related to the question, similar to how humans do).

Finally, our method currently requires a **structured output** (the answer is still essentially a word or so that’s easy to verify logically). For open-ended generation (like summarization or free-form explanation), a symbolic approach might be less directly applicable. It’s geared towards tasks with objective answers. However, it could be part of a larger system where the logical part checks consistency of a narrative generated by a neural part, for instance.

## Future Work  

The promising results of our neural-symbolic framework open up several avenues for future work to enhance and extend the approach:

**1. Integrating Knowledge Graphs and Ontologies:** One immediate next step is to incorporate external structured knowledge, such as knowledge graphs or ontologies, into the reasoning process. Our current system is largely confined to reasoning within the given text. However, many real-world QA or assistant tasks require background knowledge (e.g., knowing that “a swan is a bird” or “heart disease is related to cholesterol”). By linking the neural-symbolic reasoner to a knowledge graph, the system can fetch relevant facts or rules from a large repository when needed. For example, if a medical question involves symptoms and diseases, the system could pull medical ontology relations as additional symbolic facts. This would effectively turn the static rule base into a dynamic, queryable one. There are challenges in aligning the text with the knowledge graph symbols (entity linking), but existing work on neuro-symbolic KG reasoning ([Neurosymbolic AI for Reasoning Over Knowledge Graphs: A Survey](https://ieeexplore.ieee.org/document/10603423#:~:text=Neurosymbolic%20AI%20for%20Reasoning%20Over,which%20we%20can%20classify%20them)) provides a starting point. We would likely need to enrich the Neural-Symbolic Interface to identify mentions of known entities/concepts and map them to the KG, and then allow the inference engine to hop through the KG. This could dramatically enhance the system’s ability to answer open-domain questions. Domain-specific knowledge integration is also promising for **legal reasoning**: we could input laws or regulations as logical rules, and let the system apply them to facts described in a case (text). The ability of neuro-symbolic systems to integrate expert knowledge in structured form has been noted as a key advantage ([Q&A: Can Neuro-Symbolic AI Solve AI’s Weaknesses? | TDWI](https://tdwi.org/Articles/2024/04/08/ADV-ALL-Can-Neuro-Symbolic-AI-Solve-AI-Weaknesses.aspx#:~:text=%2A%20Learning%20efficiency.%20Neuro,knowledge%20representation%20of%20symbolic%20AI)). Our results encourage leveraging that – for instance, building a legal QA system where the neural part extracts case facts and the symbolic part applies legal rules.

**2. Adaptive and Learned Reasoning Strategies:** Currently, our hybrid controller uses fixed heuristics. A future direction is to make the controller more **adaptive or even learned**. One idea is to train a reinforcement learning agent that decides how to answer a question (neural vs symbolic actions) to maximize accuracy and efficiency. The state could be the question and perhaps initial neural predictions; the actions include invoking a reasoning step or outputting an answer. Over time, it could learn, for example, that for arithmetic questions it should always use symbolic calculation, but for very straightforward fact recall, the neural answer is enough. This is akin to how a human student might decide to mentally compute something vs. look it up. It would also be interesting to allow the controller to do **multi-step interactions**: e.g., maybe first ask the neural model to summarize the relevant parts of the story, then feed that to the logic engine, etc. Essentially, a learned policy over a repertoire of neural and symbolic “skills” could yield a flexible problem-solver. Some recent frameworks like DeepMind’s “self-ask” prompting or AI21’s routing network are inspirations here.

**3. Scaling to Unstructured Real-world Text:** While bAbI is clean, real text can be messy. Future work should test the approach on more natural datasets – for example, the Natural Questions or DROP dataset (which involves discrete reasoning like addition and date comparison). Early neuro-symbolic attempts (e.g., for DROP, researchers have used an LLM with a calculation module) show improvements. We could apply our approach to such datasets by training the parser to output, say, a pseudo-SQL or functional query representing the question. Another domain is **complex book or story QA** (like the NarrativeQA dataset), where you have to reason about events and characters over a long narrative. Our system might need to prune irrelevant info (possibly via an initial neural step) before applying logic due to long contexts.

**4. Improving Learning of Symbolic Rules:** We partially sidestepped the learning of rules by crafting them, but an important future step is **learning symbolic rules from data** to avoid manual effort and to adapt to new domains. There is rich literature on inductive logic programming (ILP) and some neural ILP (like differentiable rule learners). We could incorporate a module that hypothesizes rules when it detects patterns. For example, in the induction task we hinted at generalizing after seeing instances; a more robust system could automatically infer “If all observed swans were white, maybe all swans are white” with some confidence. In a realistic setting, one might use differentiable learning to tune the confidence of such rules. **Continual learning** of rules is also intriguing – as the system answers more queries, it might accumulate knowledge (like a knowledge graph) and rules which make it better over time. However, maintaining consistency in an evolving knowledge base is a challenge (addressed in knowledge base refinement research).

**5. Handling Uncertainty and Contradiction:** Currently, our logic engine is binary (true/false). Future work could enable **probabilistic reasoning**, making the system more robust to uncertainty or contradictions (which are common in knowledge from text). A possible direction is to integrate probabilistic graphical models or use a differentiable truth value (like ± in LNN ([Using symbolic AI for knowledge-based question answering - IBM Research](https://research.ibm.com/blog/ai-neurosymbolic-common-sense#:~:text=LNNs%20are%20a%20modification%20of,Figure%201%20illustrates%20the)) or fuzzy logic). This would allow the system to weigh evidence. For example, if two statements conflict, a probabilistic reasoner could choose the more likely one. This would also help in scenarios where the neural parser isn’t 100% certain: it could pass along probabilities instead of hard decisions, and the reasoner could in principle compute a probability of each answer (as done in DeepProbLog ([[1805.10872] DeepProbLog: Neural Probabilistic Logic Programming](https://arxiv.org/abs/1805.10872#:~:text=,exploits%20the%20full%20expressiveness%20and))). Achieving this without losing the clarity of explanation is an interesting research problem.

**6. Extending to Multi-Modal Reasoning:** Another exciting direction is to extend neural-symbolic reasoning beyond text, e.g., **vision-and-language tasks**. Imagine a small LM plus a symbolic reasoner that can also call a vision module. For instance, in a VQA setting like CLEVR, one could parse the question to logic (which NS-CL did) and query a vision-derived scene graph. Or in robotics (“Put the red block on the blue block”), parse the instruction to symbolic actions and use a planner. Our architecture could incorporate modules for different data modalities (images, tables, etc.) fairly naturally since symbolic representations are a great lingua franca for multi-modal integration (e.g., representing an image scene as logical facts about objects).

**7. Domain-Specific Applications:** We plan to apply our approach to domains like **medicine** and **law**, as they were mentioned as potential expansions. For medicine: one could use a medical LM (like PubMedBERT) for parsing patient descriptions into a structured form (symptoms, lab results), then apply clinical guidelines encoded as rules to suggest diagnoses or treatments. For law: parse legal case text into facts (who did what, when), then apply legal rules or precedents to see outcomes. These domains have well-established ontologies and rule sets (ICD for diseases, penal codes for law) that can be leveraged – a perfect use-case for neuro-symbolic synergy. The challenge is that legal/medical language is complex; the parser might need to be quite advanced (maybe using a fine-tuned medium-sized LM). Also, these decisions often involve weighing evidence rather than strict logic, tying back to the need for probabilistic reasoning.

**8. User Interaction and Explanations:** In practical deployment, a neuro-symbolic system could be interactive. Future work could allow the system to **ask clarification questions** when the input is ambiguous, akin to how a human reasoner might. This would require the system to detect ambiguity via the logic (e.g., two possible answers if a piece of info is missing) and then formulate a question (neural generation can do that). On the explanation side, we can work on making the logic traces more natural for end-users. Right now, we can list facts; we could translate that into a coherent explanation (“John went to the kitchen because he was hungry, as stated in the story.” etc.). This could increase trust in the system’s answers.

**9. Improving Efficiency for Larger Scales:** If we want to scale to very large documents or knowledge bases, we might need to optimize the reasoning process. One direction is to integrate our approach with techniques like **Retrieval-Augmented Generation (RAG)** ([Unlocking the Potential of Generative AI through Neuro-Symbolic ...](https://arxiv.org/html/2502.11269v1#:~:text=,neural%20networks%2C%20reinforcement%20learning%2C)), where the neural part first retrieves relevant pieces of text (like an open-book exam) and then our pipeline kicks in to reason over that subset. This way, if a document is 100 pages, the system doesn’t parse everything into logic (which would be huge), but rather picks, say, 10 relevant sentences to parse and reason about. This combination of IR (information retrieval) with neuro-symbolic reasoning could make the approach viable for tasks like answering questions from textbooks or even the Wikipedia (open-domain QA), with the benefit of reasoning to avoid the common LLM pitfalls of illogical answers.

**10. Integration with LLM prompting techniques:** As large LMs will remain a big part of NLP, another future avenue is to integrate our approach with them via prompting. For example, we could prompt an LLM to output a structured representation or to verify a reasoning chain produced by our system, in a feedback loop. An LLM could suggest potential rules by analyzing patterns in errors, which we then formalize. Conversely, our system could generate a chain-of-thought that an LLM then uses to phrase an answer. This kind of **hybrid AI** where large models and symbolic modules co-exist might yield both high performance and reliability.

In summary, the future work spans making the system smarter (learning to reason adaptively), broader (handling more knowledge and modalities), and more applicable (applying to domains like medicine, law, open QA). The results we obtained so far encourage these explorations because they demonstrate that even a small model, when paired with symbolic logic, can outperform purely neural approaches on tasks requiring reasoning. Extending this paradigm could lead to AI systems that are not only powerful but also **trustworthy and transparent**, aligning with the long-term goals in the AI community for explainable and robust AI ([[1905.06088] Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning](https://ar5iv.org/pdf/1905.06088#:~:text=foreseen%20by%20Valiant%2C%20two%20most,neural%20learning%20with%20symbolic%20knowledge)) ([Q&A: Can Neuro-Symbolic AI Solve AI’s Weaknesses? | TDWI](https://tdwi.org/Articles/2024/04/08/ADV-ALL-Can-Neuro-Symbolic-AI-Solve-AI-Weaknesses.aspx#:~:text=Neuro,classification%2C%20prediction%2C%20and%20contextual%20understanding)).

## Conclusion  
We presented a comprehensive study on enhancing small language models with neural-symbolic reasoning capabilities. Our proposed architecture marries a neural network’s proficiency in language understanding with a symbolic engine’s strength in logical inference, yielding a system that performs complex reasoning far beyond the reach of a standalone neural model of comparable size. Through experiments on the bAbI benchmark, we demonstrated that this hybrid approach achieves superior accuracy on a wide array of reasoning tasks – solving most with near-perfect accuracy – while also providing interpretable reasoning chains. These results underscore a key insight: **scaling reasoning ability is not the same as scaling model size**. Even a relatively compact model can exhibit strong reasoning competence when it is able to leverage symbolic structures for systematic computation.

The contributions of this work are manifold. We designed a novel neural-symbolic framework with a clear division of labor between neural and symbolic components, and an interface that allows seamless communication between the two. We showed how natural language can be converted into a logic-based representation that a reasoning engine can operate on, and how logical results can be mapped back into language – effectively bridging the gap between sub-symbolic text representations and symbolic knowledge. We also introduced a hybrid controller concept to manage the interplay, highlighting the potential for dynamic reasoning strategies. In our evaluation, we did not treat the neural and symbolic modules as black boxes; instead, we opened the hood to analyze where improvements come from and where failures occur, thus gaining insights applicable to future designs.

One of the most significant outcomes is in **interpretability and verification**. Our system can explain its answers by referencing specific facts and rules, addressing one of the critical concerns in modern AI (“Why should we trust the model’s answer?”). This aligns with the vision of **explainable AI** – systems that can not only provide answers but also justifications ([Q&A: Can Neuro-Symbolic AI Solve AI’s Weaknesses? | TDWI](https://tdwi.org/Articles/2024/04/08/ADV-ALL-Can-Neuro-Symbolic-AI-Solve-AI-Weaknesses.aspx#:~:text=,large%20data%20sets%20to%20learn)). In domains like healthcare or law, such capability is indispensable, and our approach is a step toward that goal. It suggests that instead of trying to extract explanations post-hoc from a black-box model, one can build explainability into the model’s reasoning process from the ground up via symbolic logic.

Another important implication of our work is the **potential for knowledge integration and continual learning**. Symbolic representations serve as a medium for accumulating and reusing knowledge. While neural models need extensive retraining to absorb new information, a symbolic knowledge base can be updated incrementally. Our system could, for instance, readily accept a new rule or fact at runtime and immediately take it into account during reasoning – something that is not feasible for a frozen neural model. This hints at AI systems that could incrementally grow their knowledge and reasoning skills over time, similarly to how humans learn – by adding new facts to memory and new strategies to their thinking repertoire.

However, we also recognize the challenges ahead. The positive results on synthetic tasks must be carefully validated on real-world data. Language in the wild is ambiguous and complex, and converting it to logical form without loss is itself a difficult task, traditionally the realm of computational linguistics (semantic parsing). Our system’s performance is bounded by the quality of this conversion. Nonetheless, the resurgence of interest in **large-scale semantic parsing** and the improved capabilities of language models to structure information (even if large models are used to assist the small ones) provide reason for optimism.

Looking at the broader **AI landscape**, our work contributes to the narrative that purely neural approaches and purely symbolic approaches each have limitations, but their **combination can overcome many of those limitations** ([Q&A: Can Neuro-Symbolic AI Solve AI’s Weaknesses? | TDWI](https://tdwi.org/Articles/2024/04/08/ADV-ALL-Can-Neuro-Symbolic-AI-Solve-AI-Weaknesses.aspx#:~:text=Neuro,classification%2C%20prediction%2C%20and%20contextual%20understanding)). Neural nets lack a notion of truth beyond correlation; symbolic systems lack the ability to learn from raw data. Together, the neural part can ground symbols in real data, and the symbolic part can ensure consistency and rigor of reasoning. This synergy is reminiscent of how the human mind might combine intuition and logic. By operationalizing this synergy in a computational system, we move closer to AI that can **learn, reason, and explain** – three pillars of intelligence.

In conclusion, **Neural-Symbolic Reasoning for Small LMs** is a promising direction for creating AI systems that are both **smart and trustworthy**. We have shown that even with limited computational resources (small models), one can achieve impressive reasoning depth and breadth by harnessing symbolic logic. This bodes well for making advanced AI reasoning accessible and deployable in environments where computational or data resources are limited (e.g., edge devices or specialized enterprise settings), and where interpretability is crucial. We envision that future AI will routinely integrate neural and symbolic components, leveraging the **strengths of both paradigms** to push the boundaries of what machines can understand and solve. Our work provides a step in that direction, demonstrating the tangible benefits of such an integrated approach and laying the groundwork for its extension to more complex and diverse AI challenges.